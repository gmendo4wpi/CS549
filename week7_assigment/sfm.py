import cv2
import numpy as np
import matplotlib.pyplot as plt
import struct
import open3d as o3d

image1 = cv2.imread(r'C:\Users\Gio\Desktop\tinyml\undistorted_kitchen_1.jpg', cv2.IMREAD_GRAYSCALE)
image2 = cv2.imread(r'C:\Users\Gio\Desktop\tinyml\undistorted_kitchen_2.jpg', cv2.IMREAD_GRAYSCALE)

# initialize SIFT detector
sift = cv2.SIFT_create()

# detect keypoints and descriptors
keypoints1, descriptors1 = sift.detectAndCompute(image1, None)
keypoints2, descriptors2 = sift.detectAndCompute(image2, None)

# create BFMatcher object with default params
bf = cv2.BFMatcher()
matches = bf.knnMatch(descriptors1, descriptors2, k=2)

# apply ratio test to find good matches
good_matches = []
for m, n in matches:
    if m.distance < 0.60 * n.distance:
        good_matches.append(m)

# extract location of good matches
pts1 = [keypoints1[match.queryIdx].pt for match in good_matches]
pts2 = [keypoints2[match.trainIdx].pt for match in good_matches]

pts1 = np.int32(pts1)
pts2 = np.int32(pts2)

# compute Fundamental Matrix with RANSAC
F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)
print("Fundamental Matrix F:\n", F)

# apply  mask generated by RANSAC to filter good matches
pts1 = pts1[mask.ravel() == 1]
pts2 = pts2[mask.ravel() == 1]

#  verify  epipolar constraint again with  filtered points
for i in range(len(pts1)):
    q_l = np.array([pts1[i][0], pts1[i][1], 1])
    q_r = np.array([pts2[i][0], pts2[i][1], 1])
    result = np.dot(q_r.T, np.dot(F, q_l))

# draw matches
img_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

# convert BGR to RGB for plotting
img_matches = cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB)

# Show  matches
plt.figure(figsize=(12, 6))
plt.imshow(img_matches)
plt.title('Correspondence Points Between Two Images')
plt.axis('off')
plt.show()

# function to draw lines and points
def drawlines(img1, img2, lines, pts1, pts2):
    r, c = img1.shape
    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)
    img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)
    for r, pt1, pt2 in zip(lines, pts1, pts2):
        color = tuple(np.random.randint(0, 255, 3).tolist())
        x0, y0 = map(int, [0, -r[2]/r[1] ])
        x1, y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])
        img1 = cv2.line(img1, (x0, y0), (x1, y1), color, 1)
        img1 = cv2.circle(img1, tuple(pt1), 5, color, -1)
        img2 = cv2.circle(img2, tuple(pt2), 5, color, -1)
    return img1, img2

# compute corresponding epilines 
lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2, F)
lines1 = lines1.reshape(-1,3)
img5, img6 = drawlines(image1, image2, lines1, pts1, pts2)

# display  images
plt.figure(figsize=(10, 5))
plt.subplot(121), plt.imshow(img5)
plt.subplot(122), plt.imshow(img6)
plt.show()

# camera matrix (Intrinsic parameters)
K = np.array([[3339.38546, 0, 857.9346],
              [0, 3536.30686, 201.317013],
              [0, 0, 1]])

# calculate  Essential Matrix E
E = np.dot(K.T, np.dot(F, K))
print("Essential Matrix E:\n", E)

# calculate  determinant of Essential Matrix
det_E = np.linalg.det(E)
print("Determinant of  Essential Matrix (should be close to 0):", det_E)

# perform SVD on Essential Matrix
U, S, Vt = np.linalg.svd(E)
print("Singular values of  Essential Matrix:", S)

# ensure that U and Vt have positive determinants to ensure a valid rotation matrix
if np.linalg.det(U) < 0:
    U *= -1
if np.linalg.det(Vt) < 0:
    Vt *= -1

# define W as matrix used in decomposition
W = np.array([[0, -1, 0],
              [1, 0, 0],
              [0, 0, 1]])

# calculate possible rotations
R1 = np.dot(U, np.dot(W, Vt))
R2 = np.dot(U, np.dot(W.T, Vt))

# calculate possible translations (T is up to scale)
T = U[:, 2]

# print possible rotation and translation matrices
print("Rotation Matrix R1:\n", R1)
print("Rotation Matrix R2:\n", R2)
print("Translation Vector T (up to scale):\n", T)

# projection matrix for first camera (P1)
P1 = np.dot(K, np.hstack((np.eye(3), np.zeros((3, 1)))))

# forming four possible projection matrices for second camera
P21 = np.dot(K, np.hstack((R1, T.reshape(-1, 1))))
P22 = np.dot(K, np.hstack((R1, -T.reshape(-1, 1))))
P23 = np.dot(K, np.hstack((R2, T.reshape(-1, 1))))
P24 = np.dot(K, np.hstack((R2, -T.reshape(-1, 1))))

# print projection matrices
print("First Camera Projection Matrix P1:\n", P1)
print("Second Camera Projection Matrix P21:\n", P21)
print("Second Camera Projection Matrix P22:\n", P22)
print("Second Camera Projection Matrix P23:\n", P23)
print("Second Camera Projection Matrix P24:\n", P24)

def LinearLSTriangulation(u0, P0, u1, P1):
    # create matrix A as per Hartley and Sturm's formulation
    A = np.array([
        (u0[0] * P0[2, :] - P0[0, :]),
        (u0[1] * P0[2, :] - P0[1, :]),
        (u1[0] * P1[2, :] - P1[0, :]),
        (u1[1] * P1[2, :] - P1[1, :])
    ])
    
    # use SVD to solve for X
    U, S, Vt = np.linalg.svd(A)
    X = Vt[-1]  #  solution is  last row of Vt

    # convert homogeneous coordinates to 3D
    X = X / X[3]

    return X[:3]

# function to check if a point is in front of both cameras
def is_in_front_of_both_cameras(X, P1, P2):
    # convert X to homogeneous coordinates
    X_hom = np.append(X, 1)
    
    # compute depth relative to first camera
    depth1 = np.dot(P1[2, :], X_hom)
    
    # compute depth relative to second camera
    depth2 = np.dot(P2[2, :], X_hom)
    
    return depth1 > 0 and depth2 > 0

# evaluate all P2 options to find best one
P2_options = [P21, P22, P23, P24]
valid_counts = []

for idx, P2 in enumerate(P2_options):
    valid_count = 0
    for i in range(len(pts1)):
        u0 = np.array([pts1[i][0], pts1[i][1], 1])
        u1 = np.array([pts2[i][0], pts2[i][1], 1])
        
        X = LinearLSTriangulation(u0, P1, u1, P2)
        
        if is_in_front_of_both_cameras(X, P1, P2):
            valid_count += 1
    
    valid_counts.append(valid_count)
    print(f"P2 Option {idx+1} has {valid_count} points in front of both cameras.")

# select  P2 with  highest valid_count
best_P2_index = np.argmax(valid_counts)
best_P2 = P2_options[best_P2_index]
print(f" best P2 configuration is P2 Option {best_P2_index+1}")

# triangulate all points using  best P2
triangulated_points = []

for i in range(len(pts1)):
    u0 = np.array([pts1[i][0], pts1[i][1], 1])
    u1 = np.array([pts2[i][0], pts2[i][1], 1])
    
    X = LinearLSTriangulation(u0, P1, u1, best_P2)
    triangulated_points.append(X)

triangulated_points = np.array(triangulated_points)
print("Triangulated 3D Points:\n", triangulated_points)

# 3D Visualization
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(triangulated_points[:, 0], triangulated_points[:, 1], triangulated_points[:, 2])

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.show()


def project_points(X, P):
    """ Project 3D points X using projection matrix P. """
    X_hom = np.hstack((X, np.ones((X.shape[0], 1))))  # convert to homogeneous coordinates
    proj_points_hom = np.dot(P, X_hom.T).T  # project points
    proj_points = proj_points_hom[:, :2] / proj_points_hom[:, 2].reshape(-1, 1)  # convert back to 2D
    return proj_points

# project  triangulated 3D points onto  image planes of both cameras
reprojected_pts1 = project_points(triangulated_points, P1)
reprojected_pts2 = project_points(triangulated_points, best_P2)

# calculate  re-projection error for each point
error1 = np.linalg.norm(reprojected_pts1 - pts1, axis=1)
error2 = np.linalg.norm(reprojected_pts2 - pts2, axis=1)

# mean re-projection error
mean_error1 = np.mean(error1)
mean_error2 = np.mean(error2)

print(f"Mean Re-projection Error in Image 1: {mean_error1}")
print(f"Mean Re-projection Error in Image 2: {mean_error2}")

# load undistorted images in color (BGR format)
image1_color = cv2.imread(r'C:\Users\Gio\Desktop\tinyml\undistorted_kitchen_1.jpg')
image2_color = cv2.imread(r'C:\Users\Gio\Desktop\tinyml\undistorted_kitchen_2.jpg')

def SavePCDToFile(filename, points, image, pts2d):
    """ Saves 3D points with corresponding colors to a PCD file. """
    with open(filename, 'w') as f:
        # write PCD header
        f.write('# .PCD v0.7 - Point Cloud Data file format\n')
        f.write('VERSION 0.7\n')
        f.write('FIELDS x y z rgb\n')
        f.write('SIZE 4 4 4 4\n')
        f.write('TYPE F F F F\n')
        f.write('COUNT 1 1 1 1\n')
        f.write(f'WIDTH {points.shape[0]}\n')
        f.write('HEIGHT 1\n')
        f.write('VIEWPOINT 0 0 0 1 0 0 0\n')
        f.write(f'POINTS {points.shape[0]}\n')
        f.write('DATA ascii\n')
        
        # write 3D points and their colors
        for i, (pt, pt2d) in enumerate(zip(points, pts2d)):
            x, y, z = pt
            
            # get corresponding color from color image
            color = image[int(pt2d[1]), int(pt2d[0])]
            
            # ensure color is in correct format (B, G, R)
            if len(color) == 3:
                b, g, r = color  # OpenCV loads images in BGR format
            else:
                raise ValueError(f"Unexpected color format: {color}")
            
            # convert RGB to a single float for PCD format
            rgb = (int(r) << 16) | (int(g) << 8) | int(b)
            rgb_float = struct.unpack('f', struct.pack('I', rgb))[0]
            
            # write  point and its color
            f.write(f'{x} {y} {z} {rgb_float}\n')

# example usage:
SavePCDToFile('output.pcd', triangulated_points, image1_color, pts1)

import cv2
import matplotlib.pyplot as plt
import open3d as o3d

def visualize_point_cloud(pcd_file):
    # load  PCD file
    pcd = o3d.io.read_point_cloud(pcd_file)

    # create a visualization window
    vis = o3d.visualization.Visualizer()
    vis.create_window(width=800, height=600)

    # add point cloud to  visualizer
    vis.add_geometry(pcd)

    # update renderer and poll events
    vis.poll_events()
    vis.update_renderer()

    # get view control object
    ctr = vis.get_view_control()

    # set top view
    print("Setting top view...")
    ctr.set_front([0, -1, 0])
    ctr.set_up([0, 0, 1])
    ctr.set_lookat(pcd.get_center())
    ctr.set_zoom(1.0)

    vis.poll_events()
    vis.update_renderer()

    # save top view
    vis.capture_screen_image("top_view.png")

    # set frontal view
    print("Setting frontal view...")
    ctr.set_front([0, 0, -1])
    ctr.set_up([0, -1, 0])
    ctr.set_lookat(pcd.get_center())
    ctr.set_zoom(0.8)

    vis.poll_events()
    vis.update_renderer()

    # save frontal view
    vis.capture_screen_image("frontal_view.png")

    # destroy visualization window after use
    vis.destroy_window()

    # load saved images
    top_view_image = cv2.imread("top_view.png")
    frontal_view_image = cv2.imread("frontal_view.png")

    # convert images from BGR (OpenCV default) to RGB (matplotlib default)
    top_view_image = cv2.cvtColor(top_view_image, cv2.COLOR_BGR2RGB)
    frontal_view_image = cv2.cvtColor(frontal_view_image, cv2.COLOR_BGR2RGB)

    # Display images side by side
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(top_view_image)
    plt.title('Top View')
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(frontal_view_image)
    plt.title('Frontal View')
    plt.axis('off')

    plt.show()

visualize_point_cloud('output.pcd')

